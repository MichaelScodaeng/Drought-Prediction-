{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4870ceea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current working directory of the notebook\n",
    "notebook_dir = os.getcwd()\n",
    "# Go one level up to get the project root directory\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "\n",
    "# Add the project root to the Python path if it's not already there\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Now you should be able to import your modules\n",
    "# from src.data_utils import load_config, load_and_prepare_data, split_data_chronologically\n",
    "# from src.preprocess_utils import scale_data, save_scaler, load_scaler, inverse_transform_predictions\n",
    "# from src.feature_utils import engineer_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdda90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import joblib # To load the scaler if needed for inverse transform verification\n",
    "\n",
    "# Your utility functions (make sure your VS Code/Jupyter is set up to find the 'src' directory)\n",
    "# If running the notebook from the project root, these imports should work:\n",
    "from src.data_utils import load_config, load_and_prepare_data, split_data_chronologically\n",
    "from src.preprocess_utils import scale_data, save_scaler, load_scaler, inverse_transform_predictions\n",
    "from src.feature_utils import engineer_features\n",
    "\n",
    "#reimport\n",
    "from src.data_utils import load_config, load_and_prepare_data, split_data_chronologically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672b54f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current working directory of the notebook\n",
    "notebook_dir = os.getcwd() # This will be 'c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\notebooks'\n",
    "# Go one level up to get the project root directory\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, '..')) # This will be 'c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday'\n",
    "\n",
    "# ... (sys.path.append if needed for module imports) ...\n",
    "\n",
    "# Construct the path to config.yaml relative to the project root\n",
    "config_file_path = os.path.join(project_root, \"config.yaml\")\n",
    "cfg = load_config(config_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72055ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change directory to the project root\n",
    "os.chdir(project_root)\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92a6cff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Class: Successfully imported utility functions.\n",
      "Pipeline Class: Successfully imported utility functions.\n",
      "Initializing the XGBoost global pipeline for PRE...\n",
      "Pipeline Class: Attempting to load config from: c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\config_XGBoostGlobal_SPEI.yaml\n",
      "Configuration loaded from c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\config_XGBoostGlobal_SPEI.yaml\n",
      "Pipeline Class: Artifacts for experiment 'SPEI_Forecasting_Global_XGBoost' will be saved under 'c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\run_outputs\\SPEI_Forecasting_Global_XGBoost' and 'c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\models_saved\\SPEI_Forecasting_Global_XGBoost'\n",
      "Starting the XGBoost global pipeline execution...\n",
      "\n",
      "--- Starting Pipeline Run: Experiment 'SPEI_Forecasting_Global_XGBoost' ---\n",
      "Pipeline: Loading and splitting data...\n",
      "Successfully loaded data from c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\data\\full.csv. Shape: (264201, 19)\n",
      "Converted column 'time' to datetime.\n",
      "Data sorted by ['time', 'lat', 'lon'].\n",
      "Splitting data: Train ends 2018-12-31 00:00:00, Validation ends 2020-12-31 00:00:00\n",
      "Train set shape: (253461, 19), Time range: 1901-01-16 00:00:00 to 2018-12-16 00:00:00\n",
      "Validation set shape: (4296, 19), Time range: 2019-01-16 00:00:00 to 2020-12-16 00:00:00\n",
      "Test set shape: (6444, 19), Time range: 2021-01-16 00:00:00 to 2023-12-16 00:00:00\n",
      "Pipeline: Data loaded and split. Train shape: (253461, 19)\n",
      "Pipeline: Engineering features...\n",
      "  DEBUG (create_lagged_features for other loc): Input df shape (253461, 19), head:\n",
      "      lon   lat       time        tmp  dtr   cld        tmx        tmn    pre  \\\n",
      "0  101.25  6.25 1901-01-16  25.300001  9.3  62.5  30.000000  20.700000   84.6   \n",
      "1  101.75  6.25 1901-01-16  25.800001  8.0  65.1  29.800001  21.800001  131.5   \n",
      "2   99.75  6.75 1901-01-16  27.800001  9.8  55.0  32.700000  22.900000   37.4   \n",
      "\n",
      "     wet   vap      spei   soi   dmi       pdo  nino4  nino34  nino3    pet  \n",
      "0  10.28  25.2 -0.384595 -0.09 -0.54  1.114457   0.59    0.82   0.46  108.5  \n",
      "1  13.08  26.7 -0.324920 -0.09 -0.54  1.114457   0.59    0.82   0.46  102.3  \n",
      "2   4.00  26.0 -0.612856 -0.09 -0.54  1.114457   0.59    0.82   0.46  133.3  \n",
      "Dropped 2148 rows due to NaNs after feature engineering (lags).\n",
      "  DEBUG (create_lagged_features for other loc): Input df shape (4296, 19), head:\n",
      "           lon   lat       time   tmp        dtr   cld   tmx        tmn  \\\n",
      "253461  101.25  6.25 2019-01-16  27.0   9.900001  62.8  32.0  22.100000   \n",
      "253462  101.75  6.25 2019-01-16  27.6   8.700000  65.6  32.0  23.300001   \n",
      "253463   99.75  6.75 2019-01-16  28.9  10.000000  55.0  33.9  23.900000   \n",
      "\n",
      "          pre        wet   vap      spei   soi    dmi       pdo  nino4  \\\n",
      "253461  130.6  14.099999  27.2  0.556417 -0.16  0.387 -0.381584    0.5   \n",
      "253462  198.6  18.000000  28.7  0.523803 -0.16  0.387 -0.381584    0.5   \n",
      "253463  106.5   4.880000  28.1  1.212613 -0.16  0.387 -0.381584    0.5   \n",
      "\n",
      "        nino34  nino3         pet  \n",
      "253461    0.51   0.59  114.700000  \n",
      "253462    0.51   0.59  111.600003  \n",
      "253463    0.51   0.59  136.400000  \n",
      "Dropped 2148 rows due to NaNs after feature engineering (lags).\n",
      "  DEBUG (create_lagged_features for other loc): Input df shape (6444, 19), head:\n",
      "           lon   lat       time   tmp   dtr        cld        tmx   tmn  \\\n",
      "257757  101.25  6.25 2021-01-16  25.6  10.1  63.100002  30.700000  20.6   \n",
      "257758  101.75  6.25 2021-01-16  26.2   9.1  66.200005  30.800001  21.7   \n",
      "257759   99.75  6.75 2021-01-16  27.6  10.1  55.000000  32.700000  22.6   \n",
      "\n",
      "          pre    wet        vap      spei   soi    dmi       pdo  nino4  \\\n",
      "257757  118.9  14.13  25.300001  0.435771  1.64  0.051 -0.750839  -0.93   \n",
      "257758  193.1  18.58  26.800001  0.532890  1.64  0.051 -0.750839  -0.93   \n",
      "257759   42.9   5.53  26.200000 -0.297032  1.64  0.051 -0.750839  -0.93   \n",
      "\n",
      "        nino34  nino3         pet  \n",
      "257757   -1.04   -0.8  108.500000  \n",
      "257758   -1.04   -0.8  105.400000  \n",
      "257759   -1.04   -0.8  130.200009  \n",
      "Dropped 2148 rows due to NaNs after feature engineering (lags).\n",
      "Pipeline: Feature engineering complete. Featured train shape: (251313, 37)\n",
      "Pipeline: Scaling data...\n",
      "Columns to be scaled using robust scaler: ['spei', 'tmp', 'dtr', 'cld', 'tmx', 'tmn', 'wet', 'vap', 'soi', 'dmi', 'pdo', 'nino4', 'nino34', 'nino3', 'pre', 'pet']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-09 03:27:20,274] A new study created in memory with name: no-name-00336ea8-b359-4f1b-b681-5721b9557b83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scaling complete.\n",
      "Columns:  ['lon', 'lat', 'tmp', 'dtr', 'cld', 'tmx', 'tmn', 'pre', 'wet', 'vap', 'soi', 'dmi', 'pdo', 'nino4', 'nino34', 'nino3', 'pet', 'spei_lag_12', 'tmp_lag_12', 'dtr_lag_12', 'cld_lag_12', 'tmx_lag_12', 'tmn_lag_12', 'wet_lag_12', 'vap_lag_12', 'soi_lag_12', 'dmi_lag_12', 'pdo_lag_12', 'nino4_lag_12', 'nino34_lag_12', 'nino3_lag_12', 'pre_lag_12', 'pet_lag_12', 'month', 'year']\n",
      "Scaler saved to c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\models_saved\\SPEI_Forecasting_Global_XGBoost\\models_saved/robust_scaler.joblib\n",
      "Pipeline: Data scaling and X,y preparation complete. Scaler saved to c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\models_saved\\SPEI_Forecasting_Global_XGBoost\\models_saved/robust_scaler.joblib\n",
      "Pipeline: Tuning hyperparameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-09 03:27:22,288] Trial 0 finished with value: 0.13993722196849945 and parameters: {'n_estimators': 100, 'learning_rate': 0.033924183122459914, 'max_depth': 7, 'subsample': 0.8519397429346658, 'colsample_bytree': 0.9856241516878252, 'gamma': 4.38035251043079, 'lambda': 0.011363917258752401, 'alpha': 0.7714205366166434}. Best is trial 0 with value: 0.13993722196849945.\n",
      "[I 2025-06-09 03:27:30,446] Trial 1 finished with value: 0.07696447045951871 and parameters: {'n_estimators': 400, 'learning_rate': 0.01794660345218461, 'max_depth': 9, 'subsample': 0.6704109065606505, 'colsample_bytree': 0.9696764227298453, 'gamma': 3.0979117688521858, 'lambda': 2.6454415821073284e-05, 'alpha': 3.3049483670371984e-05}. Best is trial 1 with value: 0.07696447045951871.\n",
      "[I 2025-06-09 03:27:31,045] Trial 2 finished with value: 0.32448182222306254 and parameters: {'n_estimators': 600, 'learning_rate': 0.14662688481179023, 'max_depth': 9, 'subsample': 0.8898419708793479, 'colsample_bytree': 0.5071853119148189, 'gamma': 2.577062675350314, 'lambda': 3.056681346665761e-08, 'alpha': 2.2626131103942648e-08}. Best is trial 1 with value: 0.07696447045951871.\n",
      "[I 2025-06-09 03:27:32,339] Trial 3 finished with value: 0.09676571230903844 and parameters: {'n_estimators': 100, 'learning_rate': 0.10613092751242797, 'max_depth': 6, 'subsample': 0.7446795071505374, 'colsample_bytree': 0.8193932021599463, 'gamma': 2.101008757520006, 'lambda': 6.229086046425073e-06, 'alpha': 4.612054784436844e-07}. Best is trial 1 with value: 0.07696447045951871.\n",
      "[I 2025-06-09 03:27:35,109] Trial 4 finished with value: 0.11976897437892839 and parameters: {'n_estimators': 500, 'learning_rate': 0.0344042030075682, 'max_depth': 9, 'subsample': 0.8919991035473732, 'colsample_bytree': 0.860634481169174, 'gamma': 3.6239351499443595, 'lambda': 5.5543315023406585e-06, 'alpha': 0.054249522294564256}. Best is trial 1 with value: 0.07696447045951871.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: Hyperparameter tuning complete. Best RMSE on validation: 0.0770\n",
      "Best params: {'n_estimators': 400, 'learning_rate': 0.01794660345218461, 'max_depth': 9, 'subsample': 0.6704109065606505, 'colsample_bytree': 0.9696764227298453, 'gamma': 3.0979117688521858, 'lambda': 2.6454415821073284e-05, 'alpha': 3.3049483670371984e-05}\n",
      "Pipeline: Training final model...\n",
      "Training final model on X_train (shape: (251313, 35))\n",
      "Pipeline: XGBoost model saved to c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\models_saved\\SPEI_Forecasting_Global_XGBoost\\my_xgb_model.json\n",
      "Pipeline: Final model trained and saved.\n",
      "Pipeline: Feature importance plot saved to c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\run_outputs\\SPEI_Forecasting_Global_XGBoost\\feature_importance.png\n",
      "Pipeline: Configuration used for this run saved to c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\run_outputs\\SPEI_Forecasting_Global_XGBoost\\config_used.yaml\n",
      "\n",
      "--- Final Model Evaluation ---\n",
      "Pipeline: Evaluating model on train set...\n",
      "Train Set Evaluation (Original Scale): RMSE=0.0388, MAE=0.1446, R2=0.9608\n",
      "Pipeline: Evaluating model on validation set...\n",
      "Validation Set Evaluation (Original Scale): RMSE=0.0770, MAE=0.2123, R2=0.9153\n",
      "Pipeline: Evaluating model on test set...\n",
      "Test Set Evaluation (Original Scale): RMSE=0.0565, MAE=0.1826, R2=0.9477\n",
      "Pipeline: Evaluation metrics saved to c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\run_outputs\\SPEI_Forecasting_Global_XGBoost\\evaluation_metrics.json\n",
      "Pipeline: Generating predictions on the full raw dataset...\n",
      "  Engineering features for full dataset...\n",
      "  DEBUG (create_lagged_features for other loc): Input df shape (264201, 19), head:\n",
      "        lon    lat       time        tmp        dtr        cld   tmx  \\\n",
      "0    101.25   6.25 1901-01-16  25.300001   9.300000  62.500000  30.0   \n",
      "115  100.25  16.75 1901-01-16  25.700000  13.900001  33.600002  32.7   \n",
      "116  100.75  16.75 1901-01-16  23.400000  15.300000  33.700000  31.1   \n",
      "\n",
      "           tmn   pre    wet   vap      spei   soi   dmi       pdo  nino4  \\\n",
      "0    20.700000  84.6  10.28  25.2 -0.384595 -0.09 -0.54  1.114457   0.59   \n",
      "115  18.800001   7.0   1.42  21.5  0.234143 -0.09 -0.54  1.114457   0.59   \n",
      "116  15.800000   7.4   1.67  18.5  0.305914 -0.09 -0.54  1.114457   0.59   \n",
      "\n",
      "     nino34  nino3         pet  \n",
      "0      0.82   0.46  108.500000  \n",
      "115    0.82   0.46   99.200000  \n",
      "116    0.82   0.46   96.100003  \n",
      "Dropped 2148 rows due to NaNs after feature engineering (lags).\n",
      "  Columns in full_df_featured after engineering: ['lon', 'lat', 'time', 'tmp', 'dtr', 'cld', 'tmx', 'tmn', 'pre', 'wet', 'vap', 'spei', 'soi', 'dmi', 'pdo', 'nino4', 'nino34', 'nino3', 'pet', 'spei_lag_12', 'tmp_lag_12', 'dtr_lag_12', 'cld_lag_12', 'tmx_lag_12', 'tmn_lag_12', 'wet_lag_12', 'vap_lag_12', 'soi_lag_12', 'dmi_lag_12', 'pdo_lag_12', 'nino4_lag_12', 'nino34_lag_12', 'nino3_lag_12', 'pre_lag_12', 'pet_lag_12', 'month', 'year']\n",
      "  Scaling features for full dataset...\n",
      "  Model expects columns: ['lon', 'lat', 'tmp', 'dtr', 'cld', 'tmx', 'tmn', 'pre', 'wet', 'vap', 'soi', 'dmi', 'pdo', 'nino4', 'nino34', 'nino3', 'pet', 'spei_lag_12', 'tmp_lag_12', 'dtr_lag_12', 'cld_lag_12', 'tmx_lag_12', 'tmn_lag_12', 'wet_lag_12', 'vap_lag_12', 'soi_lag_12', 'dmi_lag_12', 'pdo_lag_12', 'nino4_lag_12', 'nino34_lag_12', 'nino3_lag_12', 'pre_lag_12', 'pet_lag_12', 'month', 'year']\n",
      "  Shape of X_full_for_prediction before predict: (262053, 35)\n",
      "  Columns in X_full_for_prediction before predict: ['lon', 'lat', 'tmp', 'dtr', 'cld', 'tmx', 'tmn', 'pre', 'wet', 'vap', 'soi', 'dmi', 'pdo', 'nino4', 'nino34', 'nino3', 'pet', 'spei_lag_12', 'tmp_lag_12', 'dtr_lag_12', 'cld_lag_12', 'tmx_lag_12', 'tmn_lag_12', 'wet_lag_12', 'vap_lag_12', 'soi_lag_12', 'dmi_lag_12', 'pdo_lag_12', 'nino4_lag_12', 'nino34_lag_12', 'nino3_lag_12', 'pre_lag_12', 'pet_lag_12', 'month', 'year']\n",
      "  Making predictions...\n",
      "  Inverse transforming predictions...\n",
      "Pipeline: Full data predictions saved to c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\run_outputs\\SPEI_Forecasting_Global_XGBoost\\full_data_predictions.csv\n",
      "--- Pipeline Run Finished: Experiment 'SPEI_Forecasting_Global_XGBoost' ---\n",
      "Pipeline execution completed. Results: {'train': {'rmse': np.float64(0.038833508292022235), 'mae': np.float64(0.14457153962684374), 'r2': 0.9607609902931685}, 'validation': {'rmse': np.float64(0.07696447045951871), 'mae': np.float64(0.21227378933149513), 'r2': 0.9152993346555353}, 'test': {'rmse': np.float64(0.056467511063344626), 'mae': np.float64(0.18261177428040096), 'r2': 0.9476872016170972}}\n",
      "Generating predictions on the full dataset...\n",
      "Pipeline: Generating predictions on the full raw dataset...\n",
      "  Engineering features for full dataset...\n",
      "  DEBUG (create_lagged_features for other loc): Input df shape (262053, 37), head:\n",
      "         lon    lat       time   tmp   dtr        cld        tmx        tmn  \\\n",
      "2261   99.25  16.75 1902-01-16  23.1  15.5  33.600002  30.900000  15.400001   \n",
      "2149  101.75   6.25 1902-01-16  25.4   8.0  65.100000  29.400000  21.400000   \n",
      "2150   99.75   6.75 1902-01-16  27.2   9.8  55.000000  32.100002  22.300001   \n",
      "\n",
      "             pre        wet  ...  soi_lag_12  dmi_lag_12  pdo_lag_12  \\\n",
      "2261    4.600000   0.980000  ...       -0.09       -0.54    1.114457   \n",
      "2149  127.000000  12.139999  ...       -0.09       -0.54    1.114457   \n",
      "2150   43.600002   3.710000  ...       -0.09       -0.54    1.114457   \n",
      "\n",
      "      nino4_lag_12  nino34_lag_12  nino3_lag_12  pre_lag_12  pet_lag_12  \\\n",
      "2261          0.59           0.82          0.46         4.6        99.2   \n",
      "2149          0.59           0.82          0.46       131.5       102.3   \n",
      "2150          0.59           0.82          0.46        37.4       133.3   \n",
      "\n",
      "      month  year  \n",
      "2261      1  1902  \n",
      "2149      1  1902  \n",
      "2150      1  1902  \n",
      "\n",
      "[3 rows x 37 columns]\n",
      "Dropped 2148 rows due to NaNs after feature engineering (lags).\n",
      "  Columns in full_df_featured after engineering: ['lon', 'lat', 'time', 'tmp', 'dtr', 'cld', 'tmx', 'tmn', 'pre', 'wet', 'vap', 'spei', 'soi', 'dmi', 'pdo', 'nino4', 'nino34', 'nino3', 'pet', 'spei_lag_12', 'tmp_lag_12', 'dtr_lag_12', 'cld_lag_12', 'tmx_lag_12', 'tmn_lag_12', 'wet_lag_12', 'vap_lag_12', 'soi_lag_12', 'dmi_lag_12', 'pdo_lag_12', 'nino4_lag_12', 'nino34_lag_12', 'nino3_lag_12', 'pre_lag_12', 'pet_lag_12', 'month', 'year']\n",
      "  Scaling features for full dataset...\n",
      "  Model expects columns: ['lon', 'lat', 'tmp', 'dtr', 'cld', 'tmx', 'tmn', 'pre', 'wet', 'vap', 'soi', 'dmi', 'pdo', 'nino4', 'nino34', 'nino3', 'pet', 'spei_lag_12', 'tmp_lag_12', 'dtr_lag_12', 'cld_lag_12', 'tmx_lag_12', 'tmn_lag_12', 'wet_lag_12', 'vap_lag_12', 'soi_lag_12', 'dmi_lag_12', 'pdo_lag_12', 'nino4_lag_12', 'nino34_lag_12', 'nino3_lag_12', 'pre_lag_12', 'pet_lag_12', 'month', 'year']\n",
      "  Shape of X_full_for_prediction before predict: (259905, 35)\n",
      "  Columns in X_full_for_prediction before predict: ['lon', 'lat', 'tmp', 'dtr', 'cld', 'tmx', 'tmn', 'pre', 'wet', 'vap', 'soi', 'dmi', 'pdo', 'nino4', 'nino34', 'nino3', 'pet', 'spei_lag_12', 'tmp_lag_12', 'dtr_lag_12', 'cld_lag_12', 'tmx_lag_12', 'tmn_lag_12', 'wet_lag_12', 'vap_lag_12', 'soi_lag_12', 'dmi_lag_12', 'pdo_lag_12', 'nino4_lag_12', 'nino34_lag_12', 'nino3_lag_12', 'pre_lag_12', 'pet_lag_12', 'month', 'year']\n",
      "  Making predictions...\n",
      "  Inverse transforming predictions...\n",
      "Pipeline: Full data predictions saved to c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\run_outputs\\SPEI_Forecasting_Global_XGBoost\\full_data_predictions.csv\n",
      "Full dataset predictions generated. Sample output:\n",
      "           time   lat     lon      spei  spei_predicted        pre        tmp\n",
      "4305 1903-01-16  7.75   99.25  0.019357        0.157063  63.100002  27.500000\n",
      "4299 1903-01-16  6.75  100.25 -0.016407        0.160592  52.700000  26.900000\n",
      "4300 1903-01-16  6.75  100.75 -0.247946       -0.187772  57.800000  26.300001\n",
      "4301 1903-01-16  6.75  101.25 -0.079163       -0.188688  85.300000  26.200000\n",
      "4302 1903-01-16  7.25   99.75 -0.155871        0.025217  57.700000  27.000000\n",
      "             time    lat     lon      spei  spei_predicted  pre   tmp\n",
      "264197 2023-12-16  19.75   99.75 -0.935252       -0.759265  1.9  21.9\n",
      "264112 2023-12-16  15.75  100.75 -0.056964       -0.084803  4.4  26.5\n",
      "264179 2023-12-16  18.25  100.75 -0.750218       -0.550054  0.9  23.4\n",
      "264178 2023-12-16  18.25  100.25 -1.028655       -0.621116  0.7  24.2\n",
      "264200 2023-12-16  20.25  100.25 -0.640746       -0.912413  2.6  22.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import os\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt # For feature importance\n",
    "import json\n",
    "# Assuming these are in src/ or PYTHONPATH is set for the notebook\n",
    "try:\n",
    "    from src.data_utils import load_config, load_and_prepare_data, split_data_chronologically\n",
    "    from src.preprocess_utils import scale_data, save_scaler, load_scaler, inverse_transform_predictions\n",
    "    from src.feature_utils import engineer_features\n",
    "    print(\"Pipeline Class: Successfully imported utility functions.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Pipeline Class Error: Could not import utility functions: {e}\")\n",
    "    print(\"Ensure your PYTHONPATH is set correctly if running from a notebook, or that src is accessible.\")\n",
    "    # Define dummy functions if import fails, so class can be parsed\n",
    "    def load_config(path=None): return {}\n",
    "    def load_and_prepare_data(config=None): return None\n",
    "    def split_data_chronologically(df=None, config=None): return None, None, None\n",
    "    def engineer_features(df=None, config=None): return df\n",
    "    def scale_data(df_train=None, df_val=None, df_test=None, config=None): return None,None,None,None\n",
    "    def save_scaler(scaler=None, path=None): pass\n",
    "    def load_scaler(path=None): return None\n",
    "    def inverse_transform_predictions(df=None, target=None, scaler=None): return None\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import os\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt # For feature importance\n",
    "import json # For saving metrics\n",
    "\n",
    "# Assuming these are in src/ or PYTHONPATH is set for the notebook\n",
    "try:\n",
    "    from src.data_utils import load_config, load_and_prepare_data, split_data_chronologically\n",
    "    from src.preprocess_utils import scale_data, save_scaler, load_scaler, inverse_transform_predictions\n",
    "    from src.feature_utils import engineer_features\n",
    "    print(\"Pipeline Class: Successfully imported utility functions.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Pipeline Class Error: Could not import utility functions: {e}\")\n",
    "    print(\"Ensure your PYTHONPATH is set correctly if running from a notebook, or that src is accessible.\")\n",
    "    # Define dummy functions if import fails, so class can be parsed\n",
    "    def load_config(path=None): return {}\n",
    "    def load_and_prepare_data(config=None): return None\n",
    "    def split_data_chronologically(df=None, config=None): return None, None, None\n",
    "    def engineer_features(df=None, config=None): return df\n",
    "    def scale_data(df_train=None, df_val=None, df_test=None, config=None): return None,None,None,None\n",
    "    def save_scaler(scaler=None, path=None): pass\n",
    "    def load_scaler(path=None): return None\n",
    "    def inverse_transform_predictions(df=None, target=None, scaler=None): return None\n",
    "\n",
    "\n",
    "class XGBoostGlobalPipeline:\n",
    "    def __init__(self, config_path=\"config.yaml\"):\n",
    "        self.config_path_abs = os.path.abspath(config_path)\n",
    "        print(f\"Pipeline Class: Attempting to load config from: {self.config_path_abs}\")\n",
    "        self.cfg = load_config(self.config_path_abs)\n",
    "        \n",
    "        if not self.cfg or self.cfg.get('data',{}).get('raw_data_path') is None:\n",
    "            print(\"Pipeline Class Warning: Configuration might not have loaded correctly. Critical paths might be missing.\")\n",
    "\n",
    "        self.scaler = None\n",
    "        self.model = None\n",
    "        self.best_hyperparams = None\n",
    "        self.train_df_raw, self.val_df_raw, self.test_df_raw = None, None, None\n",
    "        self.train_df_featured, self.val_df_featured, self.test_df_featured = None, None, None\n",
    "        self.X_train, self.y_train, self.X_val, self.y_val, self.X_test, self.y_test = [None]*6\n",
    "        self.full_df_raw_for_prediction = None # Initialize\n",
    "        \n",
    "        self.experiment_name = self.cfg.get('project_setup', {}).get('project_name', 'default_experiment')\n",
    "        self.project_root_for_paths = os.path.dirname(self.config_path_abs) # Directory of config file\n",
    "\n",
    "        results_base_cfg = self.cfg.get('results',{}).get('output_base_dir', 'run_outputs')\n",
    "        self.run_output_dir = os.path.join(self.project_root_for_paths, results_base_cfg, self.experiment_name)\n",
    "        \n",
    "        models_base_dir_cfg = self.cfg.get('paths', {}).get('models_base_dir', 'models_saved') # Changed to 'paths.models_base_dir'\n",
    "        self.run_models_dir = os.path.join(self.project_root_for_paths, models_base_dir_cfg, self.experiment_name)\n",
    "\n",
    "        os.makedirs(self.run_output_dir, exist_ok=True)\n",
    "        os.makedirs(self.run_models_dir, exist_ok=True)\n",
    "        print(f\"Pipeline Class: Artifacts for experiment '{self.experiment_name}' will be saved under '{self.run_output_dir}' and '{self.run_models_dir}'\")\n",
    "\n",
    "\n",
    "    def _get_abs_path_from_config_value(self, relative_path_from_config_value):\n",
    "        if relative_path_from_config_value is None: return None\n",
    "        if os.path.isabs(relative_path_from_config_value): return relative_path_from_config_value\n",
    "        return os.path.abspath(os.path.join(self.project_root_for_paths, relative_path_from_config_value))\n",
    "\n",
    "    def load_and_split_data(self):\n",
    "        print(\"Pipeline: Loading and splitting data...\")\n",
    "        relative_raw_data_path = self.cfg.get('data', {}).get('raw_data_path')\n",
    "        if not relative_raw_data_path:\n",
    "            print(\"Pipeline Error: 'data.raw_data_path' not found in configuration.\")\n",
    "            return\n",
    "        abs_data_file_path = self._get_abs_path_from_config_value(relative_raw_data_path)\n",
    "        if not abs_data_file_path or not os.path.exists(abs_data_file_path):\n",
    "            print(f\"Pipeline Error: Data file not found at constructed absolute path: {abs_data_file_path}\")\n",
    "            return\n",
    "\n",
    "        temp_load_cfg = self.cfg.copy(); temp_load_cfg['data'] = self.cfg['data'].copy() \n",
    "        temp_load_cfg['data']['raw_data_path'] = abs_data_file_path \n",
    "        full_df_raw = load_and_prepare_data(temp_load_cfg) \n",
    "        if full_df_raw is None:\n",
    "            print(\"Pipeline Error: data_utils.load_and_prepare_data returned None.\"); return\n",
    "        self.full_df_raw_for_prediction = full_df_raw.copy() \n",
    "        self.train_df_raw, self.val_df_raw, self.test_df_raw = split_data_chronologically(full_df_raw, self.cfg)\n",
    "        print(f\"Pipeline: Data loaded and split. Train shape: {self.train_df_raw.shape if self.train_df_raw is not None else 'None'}\")\n",
    "\n",
    "    def engineer_all_features(self):\n",
    "        print(\"Pipeline: Engineering features...\")\n",
    "        if self.train_df_raw is None: raise ValueError(\"Raw training data not loaded for feature engineering.\")\n",
    "        self.train_df_featured = engineer_features(self.train_df_raw.copy(), self.cfg)\n",
    "        self.val_df_featured = engineer_features(self.val_df_raw.copy(), self.cfg)\n",
    "        self.test_df_featured = engineer_features(self.test_df_raw.copy(), self.cfg)\n",
    "        print(f\"Pipeline: Feature engineering complete. Featured train shape: {self.train_df_featured.shape if self.train_df_featured is not None else 'None'}\")\n",
    "\n",
    "    def preprocess_all_data(self):\n",
    "        print(\"Pipeline: Scaling data...\")\n",
    "        if self.train_df_featured is None: raise ValueError(\"Featured training data not available for scaling.\")\n",
    "        scaled_train, scaled_val, scaled_test, fitted_sclr = scale_data(\n",
    "            self.train_df_featured, self.val_df_featured, self.test_df_featured, self.cfg)\n",
    "        if fitted_sclr is None: raise ValueError(\"Scaler fitting failed.\")\n",
    "        self.scaler = fitted_sclr\n",
    "        \n",
    "        target_col = self.cfg['project_setup']['target_variable']\n",
    "        time_col = self.cfg['data']['time_column']\n",
    "        cols_to_drop_for_X = [target_col]\n",
    "        if time_col in scaled_train.columns: cols_to_drop_for_X.append(time_col)\n",
    "\n",
    "        self.X_train = scaled_train.drop(columns=cols_to_drop_for_X, errors='ignore')\n",
    "        print(\"Columns: \", self.X_train.columns.tolist()) # DEBUG PRINT\n",
    "        self.y_train = scaled_train[target_col]\n",
    "        self.X_val = scaled_val.drop(columns=cols_to_drop_for_X, errors='ignore')\n",
    "        self.y_val = scaled_val[target_col]\n",
    "        self.X_test = scaled_test.drop(columns=cols_to_drop_for_X, errors='ignore')\n",
    "        self.y_test = scaled_test[target_col]\n",
    "\n",
    "        scaler_filename = self.cfg.get('scaling',{}).get('scaler_filename', 'robust_scaler.joblib')\n",
    "        scaler_save_path = os.path.join(self.run_models_dir, scaler_filename) \n",
    "        save_scaler(self.scaler, scaler_save_path)\n",
    "        print(f\"Pipeline: Data scaling and X,y preparation complete. Scaler saved to {scaler_save_path}\")\n",
    "\n",
    "    def _objective_for_optuna(self, trial):\n",
    "        target_col = self.cfg['project_setup']['target_variable']\n",
    "        param = {\n",
    "            'objective': self.cfg.get('model_params', {}).get('global_xgboost', {}).get('objective', 'reg:squarederror'),\n",
    "            'eval_metric': self.cfg.get('model_params', {}).get('global_xgboost', {}).get('eval_metric', 'rmse'),\n",
    "            'tree_method': 'hist', 'random_state': self.cfg.get('project_setup', {}).get('random_seed', 42),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=100),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "            'lambda': trial.suggest_float('lambda', 1e-8, 1.0, log=True),\n",
    "            'alpha': trial.suggest_float('alpha', 1e-8, 1.0, log=True),\n",
    "        }\n",
    "        model = xgb.XGBRegressor(**param,early_stopping_rounds = 2)\n",
    "        fit_params_opt = {'verbose': False}\n",
    "        if xgb.__version__ >= '0.90': \n",
    "            fit_params_opt['eval_set'] = [(self.X_val, self.y_val)] \n",
    "        model.fit(self.X_train, self.y_train, **fit_params_opt) \n",
    "        preds_val_scaled = model.predict(self.X_val) \n",
    "        scaled_preds_val_df_opt = pd.DataFrame(preds_val_scaled, columns=[target_col], index=self.X_val.index)\n",
    "        inversed_predictions_val_opt = inverse_transform_predictions(scaled_preds_val_df_opt, target_col, self.scaler)\n",
    "        scaled_actuals_val_df_opt = pd.DataFrame(self.y_val.values, columns=[target_col], index=self.y_val.index)\n",
    "        inversed_actuals_val_opt = inverse_transform_predictions(scaled_actuals_val_df_opt, target_col, self.scaler)\n",
    "        if inversed_predictions_val_opt is None or inversed_actuals_val_opt is None: return float('inf')\n",
    "        return mean_squared_error(inversed_actuals_val_opt, inversed_predictions_val_opt)\n",
    "\n",
    "    def tune_hyperparameters(self, n_trials=50):\n",
    "        print(\"Pipeline: Tuning hyperparameters...\")\n",
    "        if self.X_train is None: raise ValueError(\"Data not preprocessed for hyperparameter tuning.\")\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(self._objective_for_optuna, n_trials=n_trials)\n",
    "        self.best_hyperparams = study.best_trial.params\n",
    "        print(f\"Pipeline: Hyperparameter tuning complete. Best RMSE on validation: {study.best_trial.value:.4f}\")\n",
    "        print(f\"Best params: {self.best_hyperparams}\")\n",
    "\n",
    "    def train_final_model(self, params=None):\n",
    "        print(\"Pipeline: Training final model...\")\n",
    "        if self.X_train is None: raise ValueError(\"Data not preprocessed for final model training.\")\n",
    "        model_params_to_use = params if params else self.best_hyperparams\n",
    "        if not model_params_to_use:\n",
    "            print(\"Pipeline Warning: No best hyperparameters. Using initial defaults from config.\")\n",
    "            model_params_to_use = self.cfg.get('model_params', {}).get('global_xgboost', {}).copy(); model_params_to_use.pop('tuning', None) \n",
    "        final_xgb_model_params = {\n",
    "            'objective': self.cfg.get('model_params', {}).get('global_xgboost', {}).get('objective', 'reg:squarederror'),\n",
    "            'eval_metric': self.cfg.get('model_params', {}).get('global_xgboost', {}).get('eval_metric', 'rmse'),\n",
    "            'tree_method': 'hist', 'random_state': self.cfg.get('project_setup', {}).get('random_seed', 42),\n",
    "            **model_params_to_use }\n",
    "        self.model = xgb.XGBRegressor(**final_xgb_model_params)\n",
    "        print(f\"Training final model on X_train (shape: {self.X_train.shape})\")\n",
    "        self.model.fit(self.X_train, self.y_train, verbose=False) \n",
    "        self.save_model() \n",
    "        print(\"Pipeline: Final model trained and saved.\")\n",
    "\n",
    "    def save_model(self):\n",
    "        if self.model is None: print(\"Pipeline Error: No model to save.\"); return\n",
    "        model_filename = self.cfg.get('model_params',{}).get('global_xgboost',{}).get('model_filename', 'xgboost_model.json')\n",
    "        model_save_path = os.path.join(self.run_models_dir, model_filename)\n",
    "        try:\n",
    "            self.model.save_model(model_save_path) \n",
    "            print(f\"Pipeline: XGBoost model saved to {model_save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Pipeline Error: Could not save XGBoost model to {model_save_path}: {e}\")\n",
    "\n",
    "    def evaluate(self, data_split='test'):\n",
    "        print(f\"Pipeline: Evaluating model on {data_split} set...\")\n",
    "        if self.model is None: print(\"Pipeline Error: Model not trained.\"); return None\n",
    "        if self.scaler is None: print(\"Pipeline Error: Scaler not available.\"); return None\n",
    "\n",
    "        X_eval, y_eval_scaled = None, None\n",
    "        if data_split == 'test' and self.X_test is not None: X_eval, y_eval_scaled = self.X_test, self.y_test\n",
    "        elif data_split == 'validation' and self.X_val is not None: X_eval, y_eval_scaled = self.X_val, self.y_val\n",
    "        elif data_split == 'train' and self.X_train is not None: X_eval, y_eval_scaled = self.X_train, self.y_train\n",
    "        else: print(f\"Pipeline Error: Data for split '{data_split}' unavailable.\"); return None\n",
    "        \n",
    "        scaled_predictions = self.model.predict(X_eval)\n",
    "        target_col = self.cfg['project_setup']['target_variable']\n",
    "        scaled_actuals_df = pd.DataFrame(y_eval_scaled.values, columns=[target_col], index=y_eval_scaled.index)\n",
    "        scaled_preds_df = pd.DataFrame(scaled_predictions, columns=[target_col], index=y_eval_scaled.index)\n",
    "        inversed_predictions = inverse_transform_predictions(scaled_preds_df, target_col, self.scaler)\n",
    "        inversed_actuals = inverse_transform_predictions(scaled_actuals_df, target_col, self.scaler)\n",
    "        \n",
    "        if inversed_predictions is not None and inversed_actuals is not None:\n",
    "            rmse = mean_squared_error(inversed_actuals, inversed_predictions)\n",
    "            mae = mean_absolute_error(inversed_actuals, inversed_predictions)\n",
    "            r2 = r2_score(inversed_actuals, inversed_predictions)\n",
    "            print(f\"{data_split.capitalize()} Set Evaluation (Original Scale): RMSE={rmse:.4f}, MAE={mae:.4f}, R2={r2:.4f}\")\n",
    "            return {'rmse': rmse, 'mae': mae, 'r2': r2}\n",
    "        else: print(f\"Pipeline Error: Could not inverse transform {data_split} predictions/actuals.\"); return None\n",
    "\n",
    "    def generate_and_save_feature_importance(self):\n",
    "        if self.model is None or not hasattr(self.model, 'feature_importances_'):\n",
    "            print(\"Pipeline Warning: Model not trained or doesn't support feature importance. Skipping plot.\")\n",
    "            return\n",
    "        if self.X_train is None or self.X_train.empty:\n",
    "            print(\"Pipeline Warning: X_train is not available. Cannot map feature importances to names. Skipping plot.\")\n",
    "            return\n",
    "\n",
    "        feat_imp_filename = self.cfg.get('results',{}).get('feature_importance_filename', 'feature_importance.png')\n",
    "        plot_save_path = os.path.join(self.run_output_dir, feat_imp_filename)\n",
    "        try:\n",
    "            fig, ax = plt.subplots(figsize=(10, max(8, len(self.X_train.columns) * 0.25))) \n",
    "            xgb.plot_importance(self.model, ax=ax, max_num_features=20, height=0.8, importance_type='weight') \n",
    "            ax.set_title(f\"XGBoost Feature Importance ({self.experiment_name})\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(plot_save_path)\n",
    "            plt.close(fig) \n",
    "            print(f\"Pipeline: Feature importance plot saved to {plot_save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Pipeline Error: Could not generate/save feature importance plot: {e}\")\n",
    "\n",
    "    def predict_on_full_data(self):\n",
    "        print(\"Pipeline: Generating predictions on the full raw dataset...\")\n",
    "        if self.model is None or self.scaler is None:\n",
    "            print(\"Pipeline Error: Model or scaler not available. Cannot make full data predictions.\")\n",
    "            return None\n",
    "        if self.full_df_raw_for_prediction is None: \n",
    "            print(\"Pipeline Error: Original full raw dataframe copy not available for prediction.\")\n",
    "            return None\n",
    "\n",
    "        print(\"  Engineering features for full dataset...\")\n",
    "        self.full_df_raw_for_prediction.sort_values(by=self.cfg['data']['time_column'], inplace=True) # Ensure time order\n",
    "        full_df_featured = engineer_features(self.full_df_raw_for_prediction.copy(), self.cfg)\n",
    "        self.full_df_raw_for_prediction = full_df_featured.copy() # Update the original copy with featured data\n",
    "        if full_df_featured.empty:\n",
    "            print(\"Pipeline Error: Feature engineering on full dataset resulted in an empty DataFrame.\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"  Columns in full_df_featured after engineering: {full_df_featured.columns.tolist()}\") # DEBUG PRINT\n",
    "\n",
    "        time_col = self.cfg['data']['time_column']\n",
    "        target_col_name = self.cfg['project_setup']['target_variable']\n",
    "        \n",
    "        scaler_feature_names = list(self.scaler.feature_names_in_) if hasattr(self.scaler, 'feature_names_in_') else []\n",
    "        if not scaler_feature_names:\n",
    "            print(\"Pipeline Error: Scaler has no feature_names_in_. Was it fitted correctly on named features?\")\n",
    "            return None\n",
    "\n",
    "        # Create a DataFrame with only the columns the scaler was fitted on, in that order\n",
    "        df_to_scale_full = pd.DataFrame(index=full_df_featured.index)\n",
    "        for col in scaler_feature_names:\n",
    "            if col in full_df_featured:\n",
    "                df_to_scale_full[col] = full_df_featured[col]\n",
    "            else:\n",
    "                # This means a column the scaler expects is missing after feature engineering the full data.\n",
    "                print(f\"Pipeline Warning: Column '{col}' (expected by scaler) not found in feature-engineered full data. Filling with NaN.\")\n",
    "                df_to_scale_full[col] = np.nan # Scaler might handle NaNs (e.g. RobustScaler ignores them) or fail.\n",
    "\n",
    "        print(\"  Scaling features for full dataset...\")\n",
    "        scaled_values_for_subset = self.scaler.transform(df_to_scale_full[scaler_feature_names])\n",
    "        scaled_subset_df = pd.DataFrame(scaled_values_for_subset, columns=scaler_feature_names, index=df_to_scale_full.index)\n",
    "\n",
    "        # Now, construct X_full_for_prediction using self.X_train.columns as the template\n",
    "        # It should contain:\n",
    "        # 1. Scaled versions of columns that were in scaler_feature_names\n",
    "        # 2. Original (unscaled) versions of other columns that are in X_train.columns (e.g. lat, lon, month, year)\n",
    "        \n",
    "        X_full_for_prediction = pd.DataFrame(index=full_df_featured.index)\n",
    "        print(f\"  Model expects columns: {self.X_train.columns.tolist()}\") # DEBUG PRINT\n",
    "\n",
    "        for col in self.X_train.columns:\n",
    "            if col in scaled_subset_df.columns: # If it was a column that got scaled\n",
    "                X_full_for_prediction[col] = scaled_subset_df[col]\n",
    "            elif col in full_df_featured.columns: # If it's an unscaled feature (like lat, lon, month, year)\n",
    "                X_full_for_prediction[col] = full_df_featured[col]\n",
    "            else:\n",
    "                print(f\"Pipeline CRITICAL Warning: Feature '{col}' expected by model not found in any processed full data source. Filling with 0.\")\n",
    "                X_full_for_prediction[col] = 0 # Fallback: not ideal\n",
    "\n",
    "        print(f\"  Shape of X_full_for_prediction before predict: {X_full_for_prediction.shape}\")\n",
    "        print(f\"  Columns in X_full_for_prediction before predict: {X_full_for_prediction.columns.tolist()}\") # DEBUG PRINT\n",
    "\n",
    "        print(\"  Making predictions...\")\n",
    "        scaled_predictions_full = self.model.predict(X_full_for_prediction)\n",
    "\n",
    "        print(\"  Inverse transforming predictions...\")\n",
    "        scaled_preds_full_df = pd.DataFrame(scaled_predictions_full, columns=[target_col_name], index=full_df_featured.index)\n",
    "        inversed_predictions_full = inverse_transform_predictions(scaled_preds_full_df, target_col_name, self.scaler)\n",
    "\n",
    "        if inversed_predictions_full is not None:\n",
    "            # Start with original full_df_raw_for_prediction to keep original columns and correct length before feature engineering NaNs were dropped\n",
    "            # Then merge predictions based on index.\n",
    "            # The index of inversed_predictions_full matches full_df_featured (after NaN drop).\n",
    "            # So, we need to add predictions to full_df_featured first, then decide what to merge back to the true original.\n",
    "            \n",
    "            output_df_with_predictions = full_df_featured.copy()\n",
    "            output_df_with_predictions[f'{target_col_name}_predicted'] = inversed_predictions_full.values # .values to align if index is slightly off\n",
    "\n",
    "            # What to save? We want original time, lat, lon, original spei (if available), and predicted spei.\n",
    "            # The full_df_raw_for_prediction has the original length and all original data.\n",
    "            # We can merge our predictions (which are on the reduced length full_df_featured index) back to full_df_raw_for_prediction.\n",
    "            \n",
    "            final_output_df = self.full_df_raw_for_prediction.copy()\n",
    "            # Add the prediction where indexes match. Non-matching will be NaN.\n",
    "            final_output_df = final_output_df.merge(\n",
    "                output_df_with_predictions[[f'{target_col_name}_predicted']], # Only the prediction column\n",
    "                left_index=True,\n",
    "                right_index=True,\n",
    "                how='left' # Keep all original rows, add predictions where available\n",
    "            )\n",
    "\n",
    "\n",
    "            cols_to_save = [time_col, 'lat', 'lon']\n",
    "            if target_col_name in final_output_df.columns: \n",
    "                cols_to_save.append(target_col_name)\n",
    "            cols_to_save.append(f'{target_col_name}_predicted')\n",
    "            for orig_pred_col in ['pre','tmp']: # Example other original columns\n",
    "                if orig_pred_col in final_output_df.columns:\n",
    "                     cols_to_save.append(orig_pred_col)\n",
    "            \n",
    "            final_output_df_subset = final_output_df[[col for col in cols_to_save if col in final_output_df.columns]]\n",
    "\n",
    "            pred_filename = self.cfg.get('results',{}).get('predictions_filename', 'full_data_predictions.csv')\n",
    "            save_path = os.path.join(self.run_output_dir, pred_filename)\n",
    "            try:\n",
    "                final_output_df_subset.to_csv(save_path, index=False)\n",
    "                print(f\"Pipeline: Full data predictions saved to {save_path}\")\n",
    "                return final_output_df_subset\n",
    "            except Exception as e:\n",
    "                print(f\"Pipeline Error: Could not save full data predictions: {e}\")\n",
    "        else:\n",
    "            print(\"Pipeline Error: Failed to inverse transform full data predictions.\")\n",
    "        return None\n",
    "\n",
    "    def save_run_config(self):\n",
    "        config_filename = self.cfg.get('results',{}).get('config_filename', 'config_used.yaml')\n",
    "        save_path = os.path.join(self.run_output_dir, config_filename)\n",
    "        try:\n",
    "            with open(save_path, 'w') as f:\n",
    "                yaml.dump(self.cfg, f, default_flow_style=False, sort_keys=False)\n",
    "            print(f\"Pipeline: Configuration used for this run saved to {save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Pipeline Error: Could not save run configuration: {e}\")\n",
    "\n",
    "    def run_full_pipeline(self, tune=True, n_trials_tuning=50):\n",
    "        print(f\"\\n--- Starting Pipeline Run: Experiment '{self.experiment_name}' ---\")\n",
    "        self.load_and_split_data()\n",
    "        if self.train_df_raw is None: print(\"Pipeline Halted: Failed at data loading/splitting.\"); return \"Failed: Data Load/Split\"\n",
    "        \n",
    "        self.engineer_all_features()\n",
    "        if self.train_df_featured is None or self.train_df_featured.empty : print(\"Pipeline Halted: Failed at feature engineering.\"); return \"Failed: Feature Engineering\"\n",
    "        \n",
    "        self.preprocess_all_data()\n",
    "        if self.X_train is None: print(\"Pipeline Halted: Failed at data preprocessing/scaling.\"); return \"Failed: Preprocessing\"\n",
    "\n",
    "        if tune:\n",
    "            self.tune_hyperparameters(n_trials=n_trials_tuning)\n",
    "        \n",
    "        self.train_final_model() \n",
    "        if self.model is None: print(\"Pipeline Halted: Failed at final model training.\"); return \"Failed: Model Training\"\n",
    "\n",
    "        self.generate_and_save_feature_importance()\n",
    "        self.save_run_config() \n",
    "\n",
    "        all_metrics = {}\n",
    "        print(\"\\n--- Final Model Evaluation ---\")\n",
    "        for split_name in ['train', 'validation', 'test']:\n",
    "            metrics = self.evaluate(data_split=split_name)\n",
    "            if metrics: all_metrics[split_name] = metrics\n",
    "        \n",
    "        metrics_filename = self.cfg.get('results',{}).get('metrics_filename', 'evaluation_metrics.json')\n",
    "        metrics_save_path = os.path.join(self.run_output_dir, metrics_filename)\n",
    "        try:\n",
    "            with open(metrics_save_path, 'w') as f:\n",
    "                json.dump(all_metrics, f, indent=4) \n",
    "            print(f\"Pipeline: Evaluation metrics saved to {metrics_save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Pipeline Error: Could not save metrics: {e}\")\n",
    "\n",
    "        self.predict_on_full_data() \n",
    "\n",
    "        print(f\"--- Pipeline Run Finished: Experiment '{self.experiment_name}' ---\")\n",
    "        return all_metrics\n",
    "\n",
    "\n",
    "print(\"Initializing the XGBoost global pipeline for PRE...\")\n",
    "\n",
    "config_file_for_pipeline = \"config_XGBoostGlobal_SPEI.yaml\"\n",
    "pipeline = XGBoostGlobalPipeline(config_path=config_file_for_pipeline)\n",
    "\n",
    "print(\"Starting the XGBoost global pipeline execution...\")\n",
    "results = pipeline.run_full_pipeline(tune=True, n_trials_tuning=5) # Small trials for testing\n",
    "print(\"Pipeline execution completed. Results:\", results)\n",
    "\n",
    "print(\"Generating predictions on the full dataset...\")\n",
    "full_predictions = pipeline.predict_on_full_data()\n",
    "print(\"Full dataset predictions generated. Sample output:\")\n",
    "print(full_predictions.head() if full_predictions is not None else \"No predictions generated.\")\n",
    "print(full_predictions.tail() if full_predictions is not None else \"No predictions generated.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e976c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: Loading and splitting data...\n",
      "Successfully loaded data from c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\data\\full.csv. Shape: (264201, 19)\n",
      "Converted column 'time' to datetime.\n",
      "Data sorted by ['time', 'lat', 'lon'].\n",
      "Splitting data: Train ends 2018-12-31 00:00:00, Validation ends 2020-12-31 00:00:00\n",
      "Train set shape: (253461, 19), Time range: 1901-01-16 00:00:00 to 2018-12-16 00:00:00\n",
      "Validation set shape: (4296, 19), Time range: 2019-01-16 00:00:00 to 2020-12-16 00:00:00\n",
      "Test set shape: (6444, 19), Time range: 2021-01-16 00:00:00 to 2023-12-16 00:00:00\n",
      "Pipeline: Data loaded and split. Train shape: (253461, 19)\n",
      "Pipeline: Engineering features...\n",
      "  DEBUG (create_lagged_features for other loc): Input df shape (253461, 19), head:\n",
      "      lon   lat       time        tmp  dtr   cld        tmx        tmn    pre  \\\n",
      "0  101.25  6.25 1901-01-16  25.300001  9.3  62.5  30.000000  20.700000   84.6   \n",
      "1  101.75  6.25 1901-01-16  25.800001  8.0  65.1  29.800001  21.800001  131.5   \n",
      "2   99.75  6.75 1901-01-16  27.800001  9.8  55.0  32.700000  22.900000   37.4   \n",
      "\n",
      "     wet   vap      spei   soi   dmi       pdo  nino4  nino34  nino3    pet  \n",
      "0  10.28  25.2 -0.384595 -0.09 -0.54  1.114457   0.59    0.82   0.46  108.5  \n",
      "1  13.08  26.7 -0.324920 -0.09 -0.54  1.114457   0.59    0.82   0.46  102.3  \n",
      "2   4.00  26.0 -0.612856 -0.09 -0.54  1.114457   0.59    0.82   0.46  133.3  \n",
      "Dropped 2148 rows due to NaNs after feature engineering (lags).\n",
      "  DEBUG (create_lagged_features for other loc): Input df shape (4296, 19), head:\n",
      "           lon   lat       time   tmp        dtr   cld   tmx        tmn  \\\n",
      "253461  101.25  6.25 2019-01-16  27.0   9.900001  62.8  32.0  22.100000   \n",
      "253462  101.75  6.25 2019-01-16  27.6   8.700000  65.6  32.0  23.300001   \n",
      "253463   99.75  6.75 2019-01-16  28.9  10.000000  55.0  33.9  23.900000   \n",
      "\n",
      "          pre        wet   vap      spei   soi    dmi       pdo  nino4  \\\n",
      "253461  130.6  14.099999  27.2  0.556417 -0.16  0.387 -0.381584    0.5   \n",
      "253462  198.6  18.000000  28.7  0.523803 -0.16  0.387 -0.381584    0.5   \n",
      "253463  106.5   4.880000  28.1  1.212613 -0.16  0.387 -0.381584    0.5   \n",
      "\n",
      "        nino34  nino3         pet  \n",
      "253461    0.51   0.59  114.700000  \n",
      "253462    0.51   0.59  111.600003  \n",
      "253463    0.51   0.59  136.400000  \n",
      "Dropped 2148 rows due to NaNs after feature engineering (lags).\n",
      "  DEBUG (create_lagged_features for other loc): Input df shape (6444, 19), head:\n",
      "           lon   lat       time   tmp   dtr        cld        tmx   tmn  \\\n",
      "257757  101.25  6.25 2021-01-16  25.6  10.1  63.100002  30.700000  20.6   \n",
      "257758  101.75  6.25 2021-01-16  26.2   9.1  66.200005  30.800001  21.7   \n",
      "257759   99.75  6.75 2021-01-16  27.6  10.1  55.000000  32.700000  22.6   \n",
      "\n",
      "          pre    wet        vap      spei   soi    dmi       pdo  nino4  \\\n",
      "257757  118.9  14.13  25.300001  0.435771  1.64  0.051 -0.750839  -0.93   \n",
      "257758  193.1  18.58  26.800001  0.532890  1.64  0.051 -0.750839  -0.93   \n",
      "257759   42.9   5.53  26.200000 -0.297032  1.64  0.051 -0.750839  -0.93   \n",
      "\n",
      "        nino34  nino3         pet  \n",
      "257757   -1.04   -0.8  108.500000  \n",
      "257758   -1.04   -0.8  105.400000  \n",
      "257759   -1.04   -0.8  130.200009  \n",
      "Dropped 2148 rows due to NaNs after feature engineering (lags).\n",
      "Pipeline: Feature engineering complete. Featured train shape: (251313, 37)\n"
     ]
    }
   ],
   "source": [
    "#test load_and_split_data()\n",
    "df = pipeline.load_and_split_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d6d455",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test engineer_all_features()\n",
    "df = pipeline.engineer_all_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ebceb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DEBUG (create_lagged_features for other loc): Input df shape (253461, 19), head:\n",
      "      lon   lat       time        tmp  dtr   cld        tmx        tmn    pre  \\\n",
      "0  101.25  6.25 1901-01-16  25.300001  9.3  62.5  30.000000  20.700000   84.6   \n",
      "1  101.75  6.25 1901-01-16  25.800001  8.0  65.1  29.800001  21.800001  131.5   \n",
      "2   99.75  6.75 1901-01-16  27.800001  9.8  55.0  32.700000  22.900000   37.4   \n",
      "\n",
      "     wet   vap      spei   soi   dmi       pdo  nino4  nino34  nino3    pet  \n",
      "0  10.28  25.2 -0.384595 -0.09 -0.54  1.114457   0.59    0.82   0.46  108.5  \n",
      "1  13.08  26.7 -0.324920 -0.09 -0.54  1.114457   0.59    0.82   0.46  102.3  \n",
      "2   4.00  26.0 -0.612856 -0.09 -0.54  1.114457   0.59    0.82   0.46  133.3  \n",
      "Dropped 2148 rows due to NaNs after feature engineering (lags).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>time</th>\n",
       "      <th>tmp</th>\n",
       "      <th>dtr</th>\n",
       "      <th>cld</th>\n",
       "      <th>tmx</th>\n",
       "      <th>tmn</th>\n",
       "      <th>pre</th>\n",
       "      <th>wet</th>\n",
       "      <th>...</th>\n",
       "      <th>soi_lag_12</th>\n",
       "      <th>dmi_lag_12</th>\n",
       "      <th>pdo_lag_12</th>\n",
       "      <th>nino4_lag_12</th>\n",
       "      <th>nino34_lag_12</th>\n",
       "      <th>nino3_lag_12</th>\n",
       "      <th>pre_lag_12</th>\n",
       "      <th>pet_lag_12</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2148</th>\n",
       "      <td>101.25</td>\n",
       "      <td>6.25</td>\n",
       "      <td>1902-01-16</td>\n",
       "      <td>24.800001</td>\n",
       "      <td>9.300000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>29.500000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>9.510000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>1.114457</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.46</td>\n",
       "      <td>84.600000</td>\n",
       "      <td>108.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2149</th>\n",
       "      <td>101.75</td>\n",
       "      <td>6.25</td>\n",
       "      <td>1902-01-16</td>\n",
       "      <td>25.400000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>65.100000</td>\n",
       "      <td>29.400000</td>\n",
       "      <td>21.400000</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>12.139999</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>1.114457</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.46</td>\n",
       "      <td>131.500000</td>\n",
       "      <td>102.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>1902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150</th>\n",
       "      <td>99.75</td>\n",
       "      <td>6.75</td>\n",
       "      <td>1902-01-16</td>\n",
       "      <td>27.200000</td>\n",
       "      <td>9.800000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>32.100002</td>\n",
       "      <td>22.300001</td>\n",
       "      <td>43.600002</td>\n",
       "      <td>3.710000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>1.114457</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.46</td>\n",
       "      <td>37.400000</td>\n",
       "      <td>133.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>1902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2151</th>\n",
       "      <td>100.25</td>\n",
       "      <td>6.75</td>\n",
       "      <td>1902-01-16</td>\n",
       "      <td>26.300001</td>\n",
       "      <td>9.800000</td>\n",
       "      <td>58.800000</td>\n",
       "      <td>31.200000</td>\n",
       "      <td>21.400000</td>\n",
       "      <td>40.500000</td>\n",
       "      <td>4.950000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>1.114457</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.46</td>\n",
       "      <td>37.800000</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2152</th>\n",
       "      <td>100.75</td>\n",
       "      <td>6.75</td>\n",
       "      <td>1902-01-16</td>\n",
       "      <td>25.600000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>30.100000</td>\n",
       "      <td>21.100000</td>\n",
       "      <td>43.100002</td>\n",
       "      <td>6.730000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>1.114457</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.46</td>\n",
       "      <td>52.100002</td>\n",
       "      <td>111.600003</td>\n",
       "      <td>1</td>\n",
       "      <td>1902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253456</th>\n",
       "      <td>99.25</td>\n",
       "      <td>19.75</td>\n",
       "      <td>2018-12-16</td>\n",
       "      <td>21.200000</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>46.100002</td>\n",
       "      <td>27.100000</td>\n",
       "      <td>15.400001</td>\n",
       "      <td>64.300000</td>\n",
       "      <td>5.280000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-0.294938</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.85</td>\n",
       "      <td>-1.09</td>\n",
       "      <td>79.600000</td>\n",
       "      <td>68.200000</td>\n",
       "      <td>12</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253457</th>\n",
       "      <td>99.75</td>\n",
       "      <td>19.75</td>\n",
       "      <td>2018-12-16</td>\n",
       "      <td>21.700000</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>46.200000</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>5.290000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-0.294938</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.85</td>\n",
       "      <td>-1.09</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>68.200000</td>\n",
       "      <td>12</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253458</th>\n",
       "      <td>100.25</td>\n",
       "      <td>19.75</td>\n",
       "      <td>2018-12-16</td>\n",
       "      <td>21.600000</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>46.800000</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>15.800000</td>\n",
       "      <td>64.200005</td>\n",
       "      <td>6.260000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-0.294938</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.85</td>\n",
       "      <td>-1.09</td>\n",
       "      <td>59.800000</td>\n",
       "      <td>68.200000</td>\n",
       "      <td>12</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253459</th>\n",
       "      <td>99.75</td>\n",
       "      <td>20.25</td>\n",
       "      <td>2018-12-16</td>\n",
       "      <td>21.400000</td>\n",
       "      <td>11.600000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>27.200000</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>91.100000</td>\n",
       "      <td>6.780000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-0.294938</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.85</td>\n",
       "      <td>-1.09</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>68.200000</td>\n",
       "      <td>12</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253460</th>\n",
       "      <td>100.25</td>\n",
       "      <td>20.25</td>\n",
       "      <td>2018-12-16</td>\n",
       "      <td>22.100000</td>\n",
       "      <td>11.400001</td>\n",
       "      <td>46.900000</td>\n",
       "      <td>27.800001</td>\n",
       "      <td>16.400000</td>\n",
       "      <td>81.700005</td>\n",
       "      <td>7.270000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-0.294938</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.85</td>\n",
       "      <td>-1.09</td>\n",
       "      <td>62.200000</td>\n",
       "      <td>68.200000</td>\n",
       "      <td>12</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>251313 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           lon    lat       time        tmp        dtr        cld        tmx  \\\n",
       "2148    101.25   6.25 1902-01-16  24.800001   9.300000  62.500000  29.500000   \n",
       "2149    101.75   6.25 1902-01-16  25.400000   8.000000  65.100000  29.400000   \n",
       "2150     99.75   6.75 1902-01-16  27.200000   9.800000  55.000000  32.100002   \n",
       "2151    100.25   6.75 1902-01-16  26.300001   9.800000  58.800000  31.200000   \n",
       "2152    100.75   6.75 1902-01-16  25.600000   9.000000  62.500000  30.100000   \n",
       "...        ...    ...        ...        ...        ...        ...        ...   \n",
       "253456   99.25  19.75 2018-12-16  21.200000  11.700000  46.100002  27.100000   \n",
       "253457   99.75  19.75 2018-12-16  21.700000  11.500000  46.200000  27.500000   \n",
       "253458  100.25  19.75 2018-12-16  21.600000  11.700000  46.800000  27.500000   \n",
       "253459   99.75  20.25 2018-12-16  21.400000  11.600000  47.000000  27.200000   \n",
       "253460  100.25  20.25 2018-12-16  22.100000  11.400001  46.900000  27.800001   \n",
       "\n",
       "              tmn         pre        wet  ...  soi_lag_12  dmi_lag_12  \\\n",
       "2148    20.200000   81.000000   9.510000  ...      -0.090      -0.540   \n",
       "2149    21.400000  127.000000  12.139999  ...      -0.090      -0.540   \n",
       "2150    22.300001   43.600002   3.710000  ...      -0.090      -0.540   \n",
       "2151    21.400000   40.500000   4.950000  ...      -0.090      -0.540   \n",
       "2152    21.100000   43.100002   6.730000  ...      -0.090      -0.540   \n",
       "...           ...         ...        ...  ...         ...         ...   \n",
       "253456  15.400001   64.300000   5.280000  ...      -0.265       0.109   \n",
       "253457  16.000000   78.000000   5.290000  ...      -0.265       0.109   \n",
       "253458  15.800000   64.200005   6.260000  ...      -0.265       0.109   \n",
       "253459  15.600000   91.100000   6.780000  ...      -0.265       0.109   \n",
       "253460  16.400000   81.700005   7.270000  ...      -0.265       0.109   \n",
       "\n",
       "        pdo_lag_12  nino4_lag_12  nino34_lag_12  nino3_lag_12  pre_lag_12  \\\n",
       "2148      1.114457          0.59           0.82          0.46   84.600000   \n",
       "2149      1.114457          0.59           0.82          0.46  131.500000   \n",
       "2150      1.114457          0.59           0.82          0.46   37.400000   \n",
       "2151      1.114457          0.59           0.82          0.46   37.800000   \n",
       "2152      1.114457          0.59           0.82          0.46   52.100002   \n",
       "...            ...           ...            ...           ...         ...   \n",
       "253456   -0.294938         -0.31          -0.85         -1.09   79.600000   \n",
       "253457   -0.294938         -0.31          -0.85         -1.09   88.000000   \n",
       "253458   -0.294938         -0.31          -0.85         -1.09   59.800000   \n",
       "253459   -0.294938         -0.31          -0.85         -1.09   78.000000   \n",
       "253460   -0.294938         -0.31          -0.85         -1.09   62.200000   \n",
       "\n",
       "        pet_lag_12  month  year  \n",
       "2148    108.500000      1  1902  \n",
       "2149    102.300000      1  1902  \n",
       "2150    133.300000      1  1902  \n",
       "2151    124.000000      1  1902  \n",
       "2152    111.600003      1  1902  \n",
       "...            ...    ...   ...  \n",
       "253456   68.200000     12  2018  \n",
       "253457   68.200000     12  2018  \n",
       "253458   68.200000     12  2018  \n",
       "253459   68.200000     12  2018  \n",
       "253460   68.200000     12  2018  \n",
       "\n",
       "[251313 rows x 37 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engineer_features(pipeline.train_df_raw, pipeline.cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79a521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing the XGBoost global pipeline for PET...\")\n",
    "config_file_for_pipeline = \"config_XGBoostGlobal_PET.yaml\"\n",
    "pipeline_pet = XGBoostGlobalPipeline(config_path=config_file_for_pipeline)\n",
    "print(\"Starting the XGBoost global pipeline execution for PET...\")\n",
    "results_pet = pipeline_pet.run_full_pipeline(tune=True, n_trials_tuning=5) # Small trials for testing\n",
    "print(\"Pipeline execution for PET completed. Results:\", results_pet)\n",
    "print(\"Generating predictions on the full dataset for PET...\")\n",
    "full_predictions_pet = pipeline_pet.predict_on_full_data()\n",
    "print(\"Full dataset predictions for PET generated. Sample output:\")\n",
    "print(full_predictions_pet.head() if full_predictions_pet is not None else \"No predictions generated.\")\n",
    "print(full_predictions_pet.tail() if full_predictions_pet is not None else \"No predictions generated.\")\n",
    "\n",
    "print(\"XGBoost global pipeline execution completed for both PRE and PET.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077e008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing the XGBoost global pipeline for SPEI...\")\n",
    "config_file_for_pipeline = \"config_XGBoostGlobal_SPEI.yaml\"\n",
    "pipeline_spei = XGBoostGlobalPipeline(config_path=config_file_for_pipeline)\n",
    "print(\"Starting the XGBoost global pipeline execution for SPEI...\")\n",
    "results_spei = pipeline_spei.run_full_pipeline(tune=True, n_trials_tuning=5) # Small trials for testing\n",
    "print(\"Pipeline execution for SPEI completed. Results:\", results_spei)\n",
    "print(\"Generating predictions on the full dataset for SPEI...\")\n",
    "full_predictions_spei = pipeline_spei.predict_on_full_data()\n",
    "print(\"Full dataset predictions for SPEI generated. Sample output:\")\n",
    "print(full_predictions_spei.head() if full_predictions_spei is not None else \"No predictions generated.\")\n",
    "print(full_predictions_spei.tail() if full_predictions_spei is not None else \"No predictions generated.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drought_lstm_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
